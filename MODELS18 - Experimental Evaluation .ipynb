{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def isNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "stats = {}\n",
    "specFiles = ['data/EDM.json','data/BMM.json','data/BC.json',\n",
    "             'data/PerR.json', 'data/TnT.json','data/TM.json']\n",
    "for specFile in specFiles:\n",
    "    jsonSpec = json.load(open(specFile))\n",
    "    stats[jsonSpec['specificationName']] = jsonSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject Systems\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "with open('subjectSystemsTable.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = ['noClafer','noConstraint', 'noAbsClafer','noIntSetReference','noSetReference','noUnboundedClaferMult',\n",
    "              'noUnobundedGroupMult', 'description']\n",
    "    writer.writerow(['system']+header)\n",
    "    for spec in stats:\n",
    "        row = []\n",
    "        row.append(spec.replace('_', ' '))\n",
    "        for field in header:\n",
    "            if field == 'description':\n",
    "                row.append('TODO')\n",
    "            else:\n",
    "                row.append(stats[spec]['originalStats'][field])\n",
    "        writer.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RQ1: Efficiency & Effectiveness\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('evalTable.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    modelHeader = ['noClaferOrig','noConstraintOrig', 'noClaferBase','noConstraintBase',\n",
    "              'noClaferFlat','noConstraintFlat','noIlpDecVar','noIlpConstraint']\n",
    "    compHeader = ['alloy', 'choco', 'bound']\n",
    "    anomalyHeader = ['DEAD_CLAFER', 'UNBOUNDED_CLAFER', 'FALSE_LB_CMULT', 'FALSE_UB_CMULT', 'FALSE_UNBOUNDED_CMULT',\n",
    "            'FALSE_LB_GMULT', 'FALSE_UB_GMULT', 'FALSE_UNBOUNDED_GMULT',\n",
    "              'LOWER_BOUNDED_ATTR', 'UPPER_BOUNDED_ATTR', 'UNBOUNDED_ATTR']\n",
    "    writer.writerow(['system'] + modelHeader + compHeader + [ n.replace('_', ' ') for n in anomalyHeader ])\n",
    "    \n",
    "    compData = {}\n",
    "    with open('compEffortData.csv', 'r') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',', quotechar='\"')\n",
    "        # next(reader, None)  # skip the headers\n",
    "        for row in reader:\n",
    "            if row[0] != 'system':\n",
    "                compData[row[0]] = {}\n",
    "                compData[row[0]]['alloy'] = row[1]\n",
    "                compData[row[0]]['choco'] = row[2]\n",
    "                compData[row[0]]['bound'] = row[3]\n",
    "    allOrigNoClafer = 0\n",
    "    allOrigNoConstraint = 0\n",
    "    allDropNoClafer = 0\n",
    "    allDropNoConstraint = 0\n",
    "    allFlatNoClafer = 0\n",
    "    allFlatNoConstraint = 0\n",
    "    allDecVar = 0\n",
    "    allIlpCstr = 0\n",
    "    allComp =  {field: 0 for field in compHeader }\n",
    "    allAnomaly = {field: 0 for field in anomalyHeader}\n",
    "    validFieldCount = defaultdict(int)\n",
    "    \n",
    "    for spec in stats:\n",
    "        anomalies = []\n",
    "        for i in range(0,len(stats[spec]['analysisResults'])):\n",
    "            anomalies.append(stats[spec]['analysisResults'][i]['anomalyTypes'])\n",
    "        counter = Counter([item for sublist in anomalies for item in sublist])\n",
    "        row = []\n",
    "        row.append(spec.replace('_', ' '))\n",
    "        allOrigNoClafer += stats[spec]['originalStats']['noClafer']\n",
    "        allOrigNoConstraint += stats[spec]['originalStats']['noConstraint']\n",
    "        allDropNoClafer += stats[spec]['afterConstraintDropStats']['noClafer']\n",
    "        allDropNoConstraint += stats[spec]['afterConstraintDropStats']['noConstraint']\n",
    "        allFlatNoClafer += stats[spec]['afterFlattenigStats']['noClafer']\n",
    "        allFlatNoConstraint += stats[spec]['afterFlattenigStats']['noConstraint']\n",
    "        allDecVar += stats[spec]['analysisResults'][1]['lbIlpRes']['model']['noDecVars']\n",
    "        allIlpCstr += stats[spec]['analysisResults'][1]['lbIlpRes']['model']['noConstraints']\n",
    "        \n",
    "        row.append(stats[spec]['originalStats']['noClafer'])\n",
    "        row.append(stats[spec]['originalStats']['noConstraint'])\n",
    "        row.append(stats[spec]['afterConstraintDropStats']['noClafer'])\n",
    "        row.append(stats[spec]['afterConstraintDropStats']['noConstraint'])\n",
    "        row.append(stats[spec]['afterFlattenigStats']['noClafer'])\n",
    "        row.append(stats[spec]['afterFlattenigStats']['noConstraint'])\n",
    "        row.append(stats[spec]['analysisResults'][1]['lbIlpRes']['model']['noDecVars'])\n",
    "        row.append(stats[spec]['analysisResults'][1]['lbIlpRes']['model']['noConstraints'])\n",
    "        for field in compHeader:\n",
    "            val = compData[spec][field]\n",
    "            if isNumber(val):\n",
    "                allComp[field] = allComp[field] + float(val)\n",
    "                validFieldCount[field] += 1\n",
    "            row.append(val)\n",
    "        for field in anomalyHeader:\n",
    "            allAnomaly[field] = counter[field] + allAnomaly[field]\n",
    "            row.append(counter[field])\n",
    "        writer.writerow(row)\n",
    "    row = []\n",
    "    row.append('All')\n",
    "    row.append(allOrigNoClafer)\n",
    "    row.append(allOrigNoConstraint)\n",
    "    row.append(allDropNoClafer)\n",
    "    row.append(allDropNoConstraint)\n",
    "    row.append(allFlatNoClafer)\n",
    "    row.append(allFlatNoConstraint)\n",
    "    row.append(allDecVar)\n",
    "    row.append(allIlpCstr)\n",
    "    for i in compHeader:\n",
    "        row.append(round(allComp[i]/validFieldCount[i],4))\n",
    "    for i in anomalyHeader:\n",
    "        row.append(allAnomaly[i])\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
